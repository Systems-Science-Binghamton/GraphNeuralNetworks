{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75d4e23",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this exercise, we will build a small graph convolutional neural network using PyTorch. \n",
    "\n",
    "## Graph Convolutional neural network \n",
    "\n",
    "A graph convolutional neural network (GCN) is a neural network designed to work with network data. It extends traditional convolutional neural networks (CNNs) that operate on grid-like data such as images to general non-grid-like graphs. \n",
    "\n",
    "We will implement a popular model proposed by Kipf and Welling in ICLR 2017: \n",
    "\n",
    "- [Kipf, Thomas and Max Welling. \"Semi-Supervised Classification with Graph Convolutional Networks.\" ArXiv abs/1609.02907 (2016)](https://openreview.net/pdf?id=SJU4ayYgl)\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "We will use the airport network as the example data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79430de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "node_table = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/skojaku/adv-net-sci-course/main/data/airport_network_v2/node_table.csv\"\n",
    ")\n",
    "edge_table = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/skojaku/adv-net-sci-course/main/data/airport_network_v2/edge_table.csv\",\n",
    "    dtype={\"src\": np.int32, \"trg\": np.int32},\n",
    ")\n",
    "src, trg = tuple(edge_table[[\"src\", \"trg\"]].values.T)\n",
    "\n",
    "rows, cols = src, trg\n",
    "nrows, ncols = node_table.shape[0], node_table.shape[0]\n",
    "A = sparse.csr_matrix(\n",
    "    (np.ones_like(rows), (rows, cols)),\n",
    "    shape=(nrows, ncols),\n",
    ").asfptype()\n",
    "\n",
    "# Symmterize and binarize\n",
    "A = A + A.T\n",
    "A.data = A.data * 0 + 1\n",
    "\n",
    "node_labels = node_table[\"region\"].values\n",
    "node_label_ids = np.unique(node_table[\"region\"].values, return_inverse=True)[1]\n",
    "num_classes = len(np.unique(node_label_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d0b815",
   "metadata": {},
   "source": [
    "## Implementing GCN \n",
    "\n",
    "The Graph Convolutional Network (GCN) carries out the convolution of node features in the following steps:\n",
    "\n",
    "1. The initial vector of node features, which has a dimension of $K$, is transformed into a new vector of dimension $K_h$ using a fully connected linear layer that can be learned. \n",
    "2. GCN performs a convolutional operation by convolving the transformed feature vector generated in step 1 with the feature vectors of its neighboring nodes. This generates a new feature vector for each node. \n",
    "3. The convoluted vectors are then passed through a linear or non-linear transformation.\n",
    "4. The steps 1--3 are repeated multiple times\n",
    "5. The final node features are fed into a linear layer, and then the last layer for a downstream application (e.g., soft-max for a classification task) \n",
    "\n",
    "Let's implement it one by one by using PyTorch.\n",
    "\n",
    "### Step 0: Preparation\n",
    "To demonstrate the effectiveness of convolution, let's use a random node feature generated by a Gaussian distribution. This feature is 100 dimensional and does not contain any information about the network. We will see that, even with this random node feature, GCN performs well in a downstream task due to its ability to leverage the network structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30891df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dim = 100\n",
    "X = torch.randn(A.shape[0], dim)  # Random features for each node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf91a2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 1: Linear transformation\n",
    "\n",
    "Let us consider that each node $i$ has a $K$-dimensional feature vector $x_i$ of continuous variables. Similarly, let us denote by $X$ the feature matrix whose $i$th row corresponds to the feature vector of node i. \n",
    "\n",
    "$$\n",
    "X := \n",
    "\\begin{bmatrix}\n",
    "x_i \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_i \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1K}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2K}\\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots\\\\\n",
    "x_{i1} & x_{i2} & \\cdots & x_{iK}\\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots\\\\\n",
    "x_{N1} & x_{N2} & \\cdots & x_{NK}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of nodes. \n",
    "\n",
    "GCN first applies a linear transformation to $X$, namely \n",
    "$$\n",
    "z^{(1)}_i = x_i W,\\quad\\text{or}\\quad Z^{(1)} = X W\n",
    "$$\n",
    "where $W$ is a $K\\times K'$ learnable matrix, and $Z^{(1)}$ is a new feature vector of nodes. The \"learnable\" means that matrix $W$ will be adjusted to improve the performance of a downstream task. To implement this step, we will use [PyTorch implementation of linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff80df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# This is the dimension of the linear layer W\n",
    "in_features = 100  # This is the dimension of the input features\n",
    "out_features = 50  # This is the dimension of the output features\n",
    "\n",
    "# Create a linear layer\n",
    "# The bias is set to False to be faithful to the original formulation. But you can set it to True if you want.\n",
    "linear_layer = torch.nn.Linear(\n",
    "    in_features=in_features, out_features=out_features, bias=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9edac71",
   "metadata": {},
   "source": [
    "You can apply the linear layer by calling it a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = linear_layer(X)\n",
    "Z1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1db05",
   "metadata": {},
   "source": [
    "### Step 2: Aggregation of node features\n",
    "\n",
    "The next step is the convolution. GCN generates a new feature vector $z_{i}^{(2)}$ for a node $i$ by aggregating the node features of the node itself (i.e., $z_i^{(1)}$) and those of its neighboring nodes. While one can use any aggregation type, such as sum, averaging, or max/min pooling, this specific GCN adopted a weighted average based on the normalized adjacency matrix with loops. Specifically, \n",
    "\n",
    "$$\n",
    "z^{(2)}_i = \\sum_{j} \\tilde{A}^{\\text{norm}}_{ij} z^{(1)} _j, \n",
    "$$\n",
    "or equivalently, in matrix form, \n",
    "$$\n",
    "Z^{(2)} = \\tilde{A}^{\\text{norm}} Z^{(1)}, \n",
    "$$\n",
    "\n",
    "Here, $\\tilde{A}^{\\text{norm}}$ represents the normalized adjacency matrix and is defined as \n",
    "\n",
    "$$ \\tilde{A}^{\\text{norm}} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}. $$\n",
    "\n",
    "Matrix $\\tilde{A} = A + I$ is the adjacency matrix with self-loops added (represented by $I$), and $\\tilde{D} = \\text{diag}\\tilde (d_1,\\ldots, \\tilde d_N)$ is the diagonal matrix that contains the degrees of the nodes in the adjacency matrix with self-loops (i.e., $\\tilde A$). \n",
    "\n",
    "(Self-loops are added to nodes for a specific reason. When a self-loop is added to a node, the node itself becomes one of its neighbors. As a result, the convolution is defined by combining the features of the node's neighbors, and we can safely disregard the inclusion of the node's features.)\n",
    "\n",
    "Now, let's implement the convolusion. We will construct the normalized adjacency matrix by using scipy and numpy. But since the node features are PyTorch Tensor, we will transform $\\tilde A$ in scipy.sparse.csr_matrix format into the corresponding format in PyTorch by using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86846b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to convert scipy sparse matrix to torch sparse matrix\n",
    "def to_torch_sparse(A):\n",
    "    \"\"\"Convert scipy sparse matrix to torch sparse matrix\"\"\"\n",
    "    Atorch = torch.sparse_csr_tensor(A.indptr, A.indices, A.data, dtype=torch.float32)\n",
    "    return Atorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0bb5b5",
   "metadata": {},
   "source": [
    "Now, let's construct the normalized adjacency matrix with self-loops ($\\tilde A$).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1070cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code -----\n",
    "# Construct the normalized adjacency matrix with self-loops\n",
    "Atilde = ...  # Adjacency matrix with self-loops\n",
    "Dtilde_inv_sqrt = ...  # Inverse square root of Dtilde\n",
    "Atilde_norm = ...  # Normalized adjacency matrix\n",
    "Atilde_norm = to_torch_sparse(Atilde_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f5e18",
   "metadata": {},
   "source": [
    "Let's perform the convolution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae12cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2 = Atilde_norm @ Z1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c3992",
   "metadata": {},
   "source": [
    "By this convolution, the new feature $Z^{(2)}$ reflects not only the original node features but also the network structures. \n",
    "\n",
    "### Step 3: Non-linear activation \n",
    "\n",
    "GCN applies a non-linear activation to the aggregated feature vector. Here, we use the [ReLu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) as the activation function by following the original implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ff2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_layer = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816d871",
   "metadata": {},
   "source": [
    "As for the linear layer, the `activation_layer` is a function that applies ReLu to its argument.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1fee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_layer(Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce90df",
   "metadata": {},
   "source": [
    "### Step 4: Stacking multiple GCN layers\n",
    "\n",
    "Let's combine the components into a single GCN layer using the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GCN_forward(Atilde_norm, X, linear_layer, activation_layer):\n",
    "    \"\"\"Forward pass\"\"\"\n",
    "    Z1 = linear_layer(X)\n",
    "    Z2 = Atilde_norm @ Z1\n",
    "    Z3 = activation_layer(Z2)\n",
    "    return Z3\n",
    "\n",
    "\n",
    "# Test\n",
    "GCN_forward(Atilde_norm, X, linear_layer, activation_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64321dfd",
   "metadata": {},
   "source": [
    "We can stack multiple GCN layers to perform the convolution multiple times. By stacking GCN layers, a focal node is able to propagate its features to its immediate neighbors through the first convolution. Through the second convolution, the focal node's features are further propagated to the neighbors of its neighbors. This enables the aggregated node feature to reflect information from a wider range of nodes in the network.\n",
    "\n",
    "Let's perform the GCN operation multiple times to see the benefit of stacking GCN layers. We will define the GCN operation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2080369",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = torch.nn.Linear(100, 100)\n",
    "activation_layer = torch.nn.ReLU()\n",
    "GCN_forward(Atilde_norm, X, linear_layer, activation_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d4719a",
   "metadata": {},
   "source": [
    "Let's perform the graph convolution multiple times. At each application of the graph convolution, we will project the 100-dimensional feature vector in 2D to see the structure encoded in the feature vectors. Note that GCN is not trained at all! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01245fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "n_conv = 5\n",
    "Z = X.clone()\n",
    "xylist = []\n",
    "for i in range(n_conv):\n",
    "    Z = GCN_forward(Atilde_norm, Z, linear_layer, activation_layer)\n",
    "    xy = LinearDiscriminantAnalysis(n_components=2).fit_transform(\n",
    "        Z.detach().numpy(), node_label_ids\n",
    "    )\n",
    "    xylist.append(xy)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"ticks\")\n",
    "fig, axes = plt.subplots(figsize=(4 * len(xylist), 4), ncols=len(xylist))\n",
    "\n",
    "for i, xy in enumerate(xylist):\n",
    "    ax = axes[i]\n",
    "    sns.scatterplot(x=xy[:, 0], y=xy[:, 1], hue=node_table[\"region\"], ax=ax)\n",
    "    ax.set_title(f\"{i}th convolution\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "\n",
    "    ax.legend().remove()\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0, frameon=False)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fd788",
   "metadata": {},
   "source": [
    "By repeating the convolution process, initially random feature vectors (0th iteration) gradually form clusters by regions because each convolution step reflects the network structure into the aggregation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664c928",
   "metadata": {},
   "source": [
    "### Step 5: Transforming the aggregated features to an output\n",
    "\n",
    "The last step is to transform the aggregated node feature vectors into an output vector (or variable). It is common that the aggregated feature is once sent to a linear layer. Then, the linear-transformed aggregated node features are sent to the final layer for a downstream application. \n",
    "We will apply the GCN for a classification task using the [Soft-max function](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcaaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_last_layer = torch.nn.Linear(\n",
    "    100, num_classes\n",
    ")  # This is the dimension of the output features\n",
    "out_layer = torch.nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ad5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_layer(second_last_layer(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099637e",
   "metadata": {},
   "source": [
    "## Assemble GCN\n",
    "\n",
    "Let's assemble all GCN components for a classification task. In this task, we will predict the region of individual airports. \n",
    "Following the PyTorch style, we will create the GCN using [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d18846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, LeakyReLU, Softmax\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "\n",
    "class GraphConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, A):\n",
    "        \"\"\"Graph Convolution Layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "         Input dimension\n",
    "        out_channels : int\n",
    "         Output dimension\n",
    "        A : scipy.csr_matrix (n_nodes, n_nodes)\n",
    "         Adjacency matrix\n",
    "        \"\"\"\n",
    "        super(GraphConv, self).__init__()\n",
    "        # Your code ----\n",
    "        self.conv_mat = ...\n",
    "        self.linear = ...\n",
    "        self.act = ...\n",
    "        # --------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor (n_nodes, in_channels)\n",
    "         Input node features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (n_nodes, out_channels)\n",
    "         Output node features\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code ----\n",
    "        z = ...\n",
    "        # --------------\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "# Test\n",
    "def test_GCN():\n",
    "    GraphConv(100, 50, A)\n",
    "\n",
    "\n",
    "test_GCN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabcfa8",
   "metadata": {},
   "source": [
    "Next, by using the GCNLayer defined above, assemble a graph neural network with two GCNLayers. The first GCN layer transforms feature vectors from `in_channel` dimensions to `hidden_channel` dimensions, and the second GCN layer transforms the `hidden_channel` dimensional vector to `out_channel` dimensional vectors. Finally, a linear layer is applied to transform the `out_channel` dimensional vector to `out_channel` dimensional vector, which is then sent to the Softmax layer to produce the final output for classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, LeakyReLU, Softmax\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channel, hidden_channel, out_channel, A):\n",
    "        \"\"\"Graph Convolution Network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channel : int\n",
    "         Input dimension\n",
    "        hidden_channel : int\n",
    "         Hidden dimension\n",
    "        out_channel : int\n",
    "         Output dimension\n",
    "        A : scipy.csr_matrix (n_nodes, n_nodes)\n",
    "         Adjacency matrix\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.hidden_channel = hidden_channel\n",
    "        self.out_channel = out_channel\n",
    "\n",
    "        # Your code ----\n",
    "        # Hint\n",
    "        # self.conv1 = ...\n",
    "        # self.conv2 = ...\n",
    "        # self.fully_connected = ...\n",
    "        # self.softmax = Softmax(dim = 1)\n",
    "        # --------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor (n_nodes, in_channels)\n",
    "         Input node features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (n_nodes, out_channels)\n",
    "         Output node features\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code ----\n",
    "        # --------------\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e975b",
   "metadata": {},
   "source": [
    "## Train GCN  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384675f8",
   "metadata": {},
   "source": [
    "We will first split the nodes into train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68627055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the node table into the train and test set.\n",
    "df = node_table.sample(frac=1, random_state=0)\n",
    "train_node_table = df.iloc[: int(len(df) * 0.8)]\n",
    "test_node_table = df.iloc[int(len(df) * 0.2) :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45539dd",
   "metadata": {},
   "source": [
    "Let's train GCN. We first construct the GCN model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(np.unique(node_labels))\n",
    "hidden_channel = 100\n",
    "in_channel = X.shape[1]\n",
    "out_channel = n_labels\n",
    "model = GCN(\n",
    "    in_channel=in_channel, hidden_channel=hidden_channel, out_channel=out_channel, A=A\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb094dd",
   "metadata": {},
   "source": [
    "Then, train the GCN by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead45e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Define training loop\n",
    "def train(model, optimizer, criterion, x_train, y_train, A, train_mask):\n",
    "    \"\"\"Train the model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "     Model\n",
    "    optimizer : torch.optim.Optimizer\n",
    "     Optimizer\n",
    "    criterion : torch.nn.modules.loss._Loss\n",
    "     Loss function\n",
    "    x_train : torch.Tensor (n_nodes, in_channels)\n",
    "     Input node features\n",
    "    y_train : torch.Tensor (n_nodes)\n",
    "     True labels\n",
    "    A : scipy.sparse.csr_matrix (n_nodes, n_nodes)\n",
    "     Adjacency matrix\n",
    "    train_mask : numpy.ndarray\n",
    "     Mask for training nodes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss.item() : float\n",
    "     Loss value\n",
    "    \"\"\"\n",
    "\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(x_train)\n",
    "\n",
    "    # Only compute loss for nodes in the training set\n",
    "    loss = criterion(output[train_mask, :], y_train[train_mask])\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return loss\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Find the indices of the train and test nodes\n",
    "train_mask = np.array(train_node_table[\"node_id\"])\n",
    "test_mask = np.array(test_node_table[\"node_id\"])\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "Y = torch.LongTensor(node_label_ids)\n",
    "\n",
    "# Number of epochs to train\n",
    "n_epochs = 1000\n",
    "pbar = tqdm(range(n_epochs))\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in pbar:\n",
    "    loss = train(model, optimizer, criterion, X, Y, A, train_mask)\n",
    "    pbar.set_description(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bac7bc",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of the model by using accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81801517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prediction_accuracy(y, yred):\n",
    "    \"\"\"Calculate prediction accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy.ndarray\n",
    "     True labels.\n",
    "    ypred : numpy.ndarray\n",
    "     Predicted labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    acc : float\n",
    "     Prediction accuracy.\n",
    "    \"\"\"\n",
    "    return float(np.sum(y == yred)) / float(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "\n",
    "# Your code ----\n",
    "# Hint\n",
    "# output = ...\n",
    "# ypred = ... # output gives a probability distribution over classes. Pick the one with the highest probability.\n",
    "# --------------\n",
    "\n",
    "acc = eval_prediction_accuracy(Y[test_mask].numpy(), ypred[test_mask])\n",
    "print(f\"Test accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea29b49",
   "metadata": {},
   "source": [
    "## Different node features\n",
    "\n",
    "While we have used a random node feature that has not contributed to any application, the GCN performs well for the classification since it leverages the network structure. This time, we give the GCN another useful node feature for classification, namely the latitude, longitude, and Time zone.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a128ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = node_table[[\"Latitude\", \"Longitude\", \"Timezone\"]].values\n",
    "X = torch.FloatTensor(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d65146",
   "metadata": {},
   "source": [
    "Let's train the GCN and check out the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a3c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28835679",
   "metadata": {},
   "source": [
    "# Different configuration of the GCN. \n",
    "\n",
    "GCN has many user-defined hyperparameters, such as the dimensionality of the hidden layers, type of non-linear activations, and number of GCN layers.\n",
    "Change these parameters and see how sensitive these hyperparameters are to the performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advnetsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
