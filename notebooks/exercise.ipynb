{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5c3506",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this exercise, we will learn how to use word2vec and adapt it to graph embedding.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "[Word2Vec](https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html) is a popular algorithm used for natural language processing (NLP) tasks, specifically for generating word embeddings. It was developed by researchers at Google and has gained significant attention in NLP. Word embeddings are dense vector representations of words, where words with similar meanings are represented by vectors close to each other in a high-dimensional space. Word2Vec learns these embeddings by learning from word co-occurrences.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vvm6CD-3eTS-jmI0VAReZg.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "We will train the word2vec using text from [Les Miserable](https://archive.org/details/lesmiserables00135gut)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://ia801604.us.archive.org/27/items/lesmiserables00135gut/lesms10.txt\"\n",
    "response = requests.get(url)\n",
    "text_data = response.text\n",
    "display(text_data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72f017",
   "metadata": {},
   "source": [
    "word2vec takes a sequence of tokens, and each token is a word, a number, or any character. To generate the token sequences, we split the text into sentences and then split each sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f705b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = nltk.sent_tokenize(text_data)\n",
    "\n",
    "# Convert sentences to words\n",
    "sentences = [utils.simple_preprocess(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc6265",
   "metadata": {},
   "source": [
    "Next, let's prepare the word2vec model. But the question is ``which word2vec?'' It is often underappreciated that [the original paper of word2vec](https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html) proposed four variants of word2vec models based on the architectures and training methods.\n",
    "\n",
    "- **Architecture: SBOW vs Skip-gram**:\n",
    "    There are two main architectures of Word2Vec: Continuous Bag of Words (CBOW) and Skip-gram. CBOW predicts the target word based on its context words, while Skip-gram predicts the context words given a target word. For example, consider the following sentence,\n",
    "    ```\n",
    "    > \"The quick brown fox jumps over the lazy dog\".\n",
    "    ```\n",
    "    CBOW trains a neural network to predict `fox` by taking the words it accompanies as input (e.g., {`brown`, `jumps`, `over`}). Skip-gram takes `fox` as input and predicts the words it accompanies.\n",
    "\n",
    "- **Training methods: Hierarchical soft-max vs. Skip-gram negative sampling**: Training word2vec has a critical computational challenge, and two heuristics are employed to overcome it. The choice is critical as it will result in substantially different embeddings. See [here](https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html) and  [here](https://proceedings.neurips.cc/paper/2021/hash/ca9541826e97c4530b07dda2eba0e013-Abstract.html) for details.\n",
    "\n",
    "Here, we will use word2vec with Skip-gram negative sampling implemented in `gensim` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Build the word2vec model\n",
    "w2v = gensim.models.Word2Vec(\n",
    "    sentences=sentences,  # input data\n",
    "    vector_size=128,  # size of the vectors\n",
    "    window=5,  # window size\n",
    "    min_count=2,  # minimum count of words\n",
    "    epochs=3,  # number of iterations\n",
    "    hs=0,  # Turn off hierarchical softmax and use negative sampling\n",
    "    sg=1,  # Use skip-gram instead of CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc64fe",
   "metadata": {},
   "source": [
    "The generated vectors can be retrieved by `w2v.wv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4777e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv[\"jean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f3730",
   "metadata": {},
   "source": [
    "Let's visualize the embedding of characters. Because the original embedding is high dimensional, we will project it into 2D using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec47b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get characters' embedding\n",
    "import numpy as np\n",
    "\n",
    "les_miserables_characters = [\n",
    "    \"myriel\",\n",
    "    \"napoleon\",\n",
    "    \"mllebaptistine\",\n",
    "    \"mmemagloire\",\n",
    "    \"countessdelo\",\n",
    "    \"geborand\",\n",
    "    \"champtercier\",\n",
    "    \"cravatte\",\n",
    "    \"labarre\",\n",
    "    \"jean\",\n",
    "    \"marguerite\",\n",
    "    \"mmeder\",\n",
    "    \"isabeau\",\n",
    "    \"gervais\",\n",
    "    \"tholomyes\",\n",
    "    \"listolier\",\n",
    "    \"fameuil\",\n",
    "    \"blacheville\",\n",
    "    \"favourite\",\n",
    "    \"dahlia\",\n",
    "    \"zephine\",\n",
    "    \"fantine\",\n",
    "    \"mmethenardier\",\n",
    "    \"thenardier\",\n",
    "    \"cosette\",\n",
    "    \"javert\",\n",
    "    \"fauchelevent\",\n",
    "    \"bamatabois\",\n",
    "    \"perpetue\",\n",
    "    \"simplice\",\n",
    "    \"scaufflaire\",\n",
    "    \"judge\",\n",
    "    \"champmathieu\",\n",
    "    \"brevet\",\n",
    "    \"chenildieu\",\n",
    "    \"cochepaille\",\n",
    "    \"pontmercy\",\n",
    "    \"boulatruelle\",\n",
    "    \"eponine\",\n",
    "    \"anzelma\",\n",
    "    \"motherinnocent\",\n",
    "    \"gribier\",\n",
    "    \"jondrette\",\n",
    "    \"mmeburgon\",\n",
    "    \"gavroche\",\n",
    "    \"gillenormand\",\n",
    "    \"magnon\",\n",
    "    \"mllegillenormand\",\n",
    "    \"mmepontmercy\",\n",
    "    \"mllevaubois\",\n",
    "    \"ltgillenormand\",\n",
    "    \"marius\",\n",
    "    \"baronesst\",\n",
    "    \"mabeuf\",\n",
    "    \"enjolras\",\n",
    "    \"combeferre\",\n",
    "    \"prouvaire\",\n",
    "    \"feuilly\",\n",
    "    \"courfeyrac\",\n",
    "    \"bahorel\",\n",
    "    \"bossuet\",\n",
    "    \"joly\",\n",
    "    \"grantaire\",\n",
    "    \"motherplutarch\",\n",
    "    \"gueulemer\",\n",
    "    \"babet\",\n",
    "    \"claquesous\",\n",
    "    \"montparnasse\",\n",
    "    \"toussaint\",\n",
    "    \"brujon\",\n",
    "    \"mmehucheloup\",\n",
    "]\n",
    "# Filter out characters that are not in the vocabulary\n",
    "les_miserables_characters = [c for c in les_miserables_characters if c in w2v.wv]\n",
    "# Get the embedding of the characters\n",
    "emb = np.vstack([w2v.wv[c] for c in les_miserables_characters if c in w2v.wv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "xy = PCA(n_components=2).fit_transform(emb)\n",
    "\n",
    "# Plot the characters\n",
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=xy[:, 0], y=xy[:, 1])\n",
    "for i, c in enumerate(les_miserables_characters):\n",
    "    plt.annotate(c, xy[i], fontsize=8)\n",
    "\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7841f2e",
   "metadata": {},
   "source": [
    "Although PCA is a valuable tool, it is known to be highly sensitive to outliers. This sensitivity often leads to projections that are not useful, with only a few points deviating significantly from the majority of points.\n",
    "\n",
    "[UMAP](https://umap-learn.readthedocs.io/en/latest/) is a non-linear dimensionality reduction method that produces a more compact projection. It can take data in non-Euclidean metric space and project it to Euclidean space. For word2vec, the embedding is not a metric space but is often considered a spherical embedding, where the angle between two vectors represents the similarity of the vectors. For this reason, often cosine similarity is adopted. With UMAP, you can specify the metric space for word embedding by using the `metric` argument. A common choice for word2vec is `cosine.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# Reduce dimensionality\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=5, metric=\"cosine\")\n",
    "xy = reducer.fit_transform(emb)\n",
    "\n",
    "\n",
    "# Plot the characters\n",
    "# %%\n",
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    ")\n",
    "for i, c in enumerate(les_miserables_characters):\n",
    "    plt.annotate(c, xy[i], fontsize=8)\n",
    "\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef43a45",
   "metadata": {},
   "source": [
    "Note that the UMAP projection may not accurately represent the data. Use it solely to generate hypotheses. See this [active discussion](https://twitter.com/lpachter/status/1431325969411821572?s=12) for details.\n",
    "\n",
    "\n",
    "## Adapting word2vec for graph embedding: node2vec\n",
    "\n",
    "word2vec can learn from a sequence of tokens. The tokens can be any entities, such as words, characters, numbers, and nodes. Thus, word2vec can generate *node embedding* if we can create a sequence of nodes from a network.\n",
    "\n",
    "One natural way to generate node sequences is *random walks*. In a random walk process, a walker starts from a node and traverses the network by randomly selecting a neighboring node at each time step.\n",
    "\n",
    "Let's implement a function to generate a node sequence based on a random walk from a given network and a given starting point. For the sake of simplicity, let's assume that the network is unweighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edfaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(A, start_node_id, walk_length):\n",
    "    \"\"\"Random walk on a graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix\n",
    "        Adjacency matrix of the graph.\n",
    "    start_node_id : int\n",
    "        Id of the starting node.\n",
    "    walk_length : int\n",
    "        Length of the walk.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    visited_nodes : list\n",
    "        List of visited nodes.\n",
    "    \"\"\"\n",
    "    visited_nodes = [start_node_id]\n",
    "    current_node_id = start_node_id\n",
    "\n",
    "    for _ in range(walk_length):\n",
    "        # Your code here ----------------------\n",
    "        # Get the neighbors of the current node\n",
    "        # Hint:\n",
    "        # - A.indices and A.indptr are convenient to get the neighbors\n",
    "        # - The edge weight is in A.data\n",
    "        # - np.random.choice is convenient to sample a node from a neighbor with probability proportional to the edge weight\n",
    "\n",
    "        # -------------------------------------\n",
    "\n",
    "    return visited_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac823f74",
   "metadata": {},
   "source": [
    "Let's generate the node sequences to train word2vec. We will use the network of characters in Les Miserables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "node_table = pd.read_csv(\"../data/lesmiserable/nodes.csv\")\n",
    "edge_table = pd.read_csv(\"../data/lesmiserable/edges.csv\")\n",
    "\n",
    "rows, cols = edge_table[\"src\"].values, edge_table[\"trg\"].values\n",
    "weight = edge_table[\"weight\"].values\n",
    "nrows, ncols = node_table.shape[0], node_table.shape[0]\n",
    "A = sparse.csr_matrix(\n",
    "    (weight, (rows, cols)),\n",
    "    shape=(nrows, ncols),\n",
    ")\n",
    "A = A + A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to generate a random walk. Generate 20 node sequences of length 80 for each node, totalling node_table.shape[0] * 20 sequences\n",
    "n_walkers = 20\n",
    "walk_length = 80\n",
    "\n",
    "# Your code here ----------------------\n",
    "# Hint:\n",
    "# - Use the random_walk function\n",
    "# - Use a for loop to iterate over all the nodes\n",
    "# - Use another for loop to iterate over the walkers\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "# Generate an embedding from word2vec\n",
    "# Build the word2vec model\n",
    "w2v = gensim.models.Word2Vec(\n",
    "    sentences=sentences,  # input data\n",
    "    vector_size=128,  # size of the vectors\n",
    "    window=5,  # window size\n",
    "    min_count=2,  # minimum count of words\n",
    "    epochs=3,  # number of iterations\n",
    "    hs=0,  # Turn off hierarchical softmax and use negative sampling\n",
    "    sg=1,  # Use skip-gram instead of CBOW\n",
    ")\n",
    "\n",
    "# Retrieve the embedding vectors and pack them into 2D numpy array\n",
    "emb = np.vstack([w2v.wv[c] for c in np.arange(node_table.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the UMAP of the generated embedding\n",
    "\n",
    "# Reduce dimensionality\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, min_dist=0.5, n_neighbors=30)\n",
    "xy = reducer.fit_transform(emb)\n",
    "\n",
    "\n",
    "# Plot the characters\n",
    "# %%\n",
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=0.9)\n",
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    ")\n",
    "\n",
    "for i in range(node_table.shape[0]):\n",
    "    plt.annotate(node_table.iloc[i][\"name\"], xy[i], fontsize=8)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268286fa",
   "metadata": {},
   "source": [
    "This algorithm, which uses the skip-gram negative sampling word2vec to generate graph embedding, is known as [node2vec](https://dl.acm.org/doi/10.1145/2939672.2939754).\n",
    "\n",
    "## DeepWalk\n",
    "\n",
    "The idea of using word2vec in conjunction with random walks for graph embedding was initially showcased by [DeepWalk](https:// dl.acm.org/doi/10.1145/2623330.2623732). DeepWalk predates node2vec and operates within a similar framework. The primary distinction is that DeepWalk employs hierarchical softmax for training word2vec, as opposed to negative sampling.\n",
    "\n",
    "Let's implement DeepWalk and compare the embedding. It is often convenient to wrap all related functions and variables into a function or a class. Here, we will implement DeepWalk class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10cdb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepWalk:\n",
    "    def __init__(self, window_length=10, n_walkers=50, walk_length=80):\n",
    "        \"\"\"DeepWalk class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        p : float\n",
    "          Walk bias parameter\n",
    "        q : float\n",
    "          In-out parameter\n",
    "        \"\"\"\n",
    "        self.window_length = window_length\n",
    "        self.n_walks = n_walkers\n",
    "        self.walk_length = walk_length\n",
    "        return self\n",
    "\n",
    "    def embed(self, A, dim):\n",
    "        \"\"\"Embed nodes in a graph\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        A : scipy sparse matrix\n",
    "          Adjacency matrix\n",
    "        dim : int\n",
    "          Dimension of embeddings\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        emb : numpy array (n_nodes, dim)\n",
    "          Embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code to generate embeddings\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549ab58",
   "metadata": {},
   "source": [
    "Generate the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596bb51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = DeepWalk().embed(A, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb00c4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Generate the UMAP of the generated embedding\n",
    "\n",
    "# Reduce dimensionality\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, min_dist=0.5, n_neighbors=30)\n",
    "xy = reducer.fit_transform(emb)\n",
    "\n",
    "# Plot the characters\n",
    "# %%\n",
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=0.9)\n",
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=xy[:, 0], y=xy[:, 1])\n",
    "\n",
    "for i in range(node_table.shape[0]):\n",
    "    plt.annotate(node_table.iloc[i][\"name\"], xy[i], fontsize=8)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf35a79",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advnetsci",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
